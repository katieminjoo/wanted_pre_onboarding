{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugGR7pnI4WSe"
   },
   "source": [
    "# Week3_2 Assignment\n",
    "\n",
    "## [BASIC](#Basic) \n",
    "- 한국어 코퍼스를 로드해 **WordPiece Tokenzier를 학습**시킬 수 있다.\n",
    "- 학습된 모델을 로드해 **encoding과 decoding을 수행**할 수 있다. \n",
    "\n",
    "\n",
    "\n",
    "### Reference\n",
    "- [BertWordPieceTokenizer 학습 소개 한국어 블로그](https://monologg.kr/2020/04/27/wordpiece-vocab/)\n",
    "- [huggingface python train tutorial](https://github.com/huggingface/tokenizers/blob/master/bindings/python/examples/train_bert_wordpiece.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:02:24.193088Z",
     "start_time": "2022-02-08T13:02:23.939474Z"
    },
    "id": "HlEy3xfY4WSh"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T12:20:56.576336Z",
     "start_time": "2022-02-08T12:20:56.574697Z"
    },
    "id": "cBrr7-gt4jnf"
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:03:10.552879Z",
     "start_time": "2022-02-08T13:03:09.944672Z"
    },
    "id": "6mC9lhsJ4WSh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:03:11.318888Z",
     "start_time": "2022-02-08T13:03:11.314522Z"
    },
    "id": "17g7UZ5g4WSi"
   },
   "outputs": [],
   "source": [
    "# seed\n",
    "seed = 7777\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:03:12.552228Z",
     "start_time": "2022-02-08T13:03:12.517904Z"
    },
    "id": "v3UlC7Jn4WSi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# available GPUs : 1\n",
      "GPU name : GeForce RTX 3090\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device type\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoHa39WF5AFt"
   },
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHs8_LU04WSj"
   },
   "source": [
    "### 데이터 다운로드\n",
    "- 내 구글 드라이브에 데이터를 다운 받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있다. \n",
    "- [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "4TWfB_xf5AFy"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPoV_wON5AF1"
   },
   "outputs": [],
   "source": [
    "cd # 데이터 다운로드할 위치 입력"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:05:10.693203Z",
     "start_time": "2022-02-08T13:05:05.809566Z"
    },
    "id": "4QPBJ6UZ4WSj"
   },
   "source": [
    "# 한국어 위키피디아 데이터 (토크나이즈되지 않은 텍스트) 로드\n",
    "# !pip install gdown\n",
    "!gdown https://drive.google.com/u/0/uc?id=1kUecR7xO7bsHFmUI6AExtY5u2XXlObOG\n",
    "!unzip processed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "qKjNsKAk5AF5",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-078b814d1e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_CUR_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"My current directory : {_CUR_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_DATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CUR_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"processed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "_CUR_DIR = os.path.abspath(os.curdir)\n",
    "print(f\"My current directory : {_CUR_DIR}\")\n",
    "_DATA_DIR = os.path.join(_CUR_DIR, \"processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIO4LFu67z72"
   },
   "source": [
    "### 한국어 위키피디아 코퍼스로 WordPiece tokenizer 학습\n",
    "- 한국어 위키 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:07:46.216505Z",
     "start_time": "2022-02-08T13:07:41.735392Z"
    },
    "id": "xkjqztIA4WSl"
   },
   "outputs": [],
   "source": [
    "# processed_wiki_ko.txt 파일 불러오기\n",
    "\n",
    "# 코퍼스 로드\n",
    "docs = []\n",
    "with open('./processed/processed_wiki_ko.txt','r') as f:\n",
    "    docs = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T13:07:55.901375Z",
     "start_time": "2022-02-08T13:07:55.898177Z"
    },
    "id": "WAKB6bbt4WSl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# wiki documents: 311,237\n"
     ]
    }
   ],
   "source": [
    "print(f\"# wiki documents: {len(docs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T15:04:42.728698Z",
     "start_time": "2022-02-08T15:04:42.725712Z"
    },
    "id": "rsSDiCKC7z73"
   },
   "outputs": [],
   "source": [
    "# Word Piece Tokenizer 인스턴스 생성\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # 악센트가 있는 character의 악센트를 제거하려면? (ex. é → e)\n",
    "    lowercase=False, # 한국어는 대소문자가 없는데 소문자 변환이 필요한지?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T15:07:44.027617Z",
     "start_time": "2022-02-08T15:06:26.871651Z"
    },
    "id": "d2XCyb-R7z73"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "# files: 'processed_wiki_ko.txt'\n",
    "# vocab_size: 30,000\n",
    "# min_frequency: 2\n",
    "# special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "# limit_alphabet: 1,000\n",
    "# wordpieces_prefix: '##'\n",
    "\n",
    "tokenizer.train(\n",
    "    files = ['./processed/processed_wiki_ko.txt'],\n",
    "    vocab_size = 30000,\n",
    "    min_frequency = 2,\n",
    "    show_progress = True,\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet = 1000,\n",
    "    wordpieces_prefix = '##',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T15:09:05.258756Z",
     "start_time": "2022-02-08T15:09:05.245393Z"
    },
    "id": "XP_TSZ4W7z74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\".\", \"wordpiece\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C44J9vSg7z74"
   },
   "source": [
    "### Encoding\n",
    "- 저장된 토크나이즈 파일을 로드해 `BertWordPieceTokenizer` 인스턴스를 생성하고 다음을 수행하자. \n",
    "    - 사전(vocab)의 단어 개수를 출력\n",
    "    - 문장을 토크나이징한 후 토큰 id와 토큰 string을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ipYjhaM05AGH"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab = './wordpiece-vocab.txt',\n",
    "    lowercase = False,\n",
    "    strip_accents = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "q5MsfhF_5AGI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 단어 개수 출력\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Yp9kVECa5AGJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 7864, 20862, 16, 509, 3371, 5566, 2778, 5757, 16, 3]\n",
      "['[CLS]', '안녕', '##하세요', '.', '버', '##트를', '사용한', '모델', '##입니다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"안녕하세요. 버트를 사용한 모델입니다.\"\n",
    "encoded = tokenizer.encode(text)\n",
    "\n",
    "# 토크나이즈한 후 토큰의 id를 출력하라 \n",
    "token_ids = encoded.ids\n",
    "print(token_ids)\n",
    "\n",
    "# 토크나이즈한 후 각 토큰(string)을 출력하라.\n",
    "tokens = encoded.tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T15:12:06.621489Z",
     "start_time": "2022-02-08T15:12:06.618447Z"
    },
    "id": "mzrnMuSr7z74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 신조어를 토크나이징할 수 있는지 테스트해보자.\n",
    "text = \"어쩔티비\"\n",
    "tokens = tokenizer.encode(text).tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ewQ4JTbR5AGM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17386, 1063, 3]\n",
      "['[CLS]', '베이커', '##리', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 사전에 없는 단어는 어떻게 토크나이즈 되는가?\n",
    "text = \"베이커리\"\n",
    "unknown_token_ids = tokenizer.encode(text).ids # 토큰 id\n",
    "unknown_tokens = tokenizer.encode(text).tokens # 토큰\n",
    "print(unknown_token_ids)\n",
    "print(unknown_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgO1JJyM5AGN"
   },
   "source": [
    "### Decoding\n",
    "- 토큰 id를 원래 문장으로 디코딩하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ew4gUQ-F5AGN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요. 버트를 사용한 모델입니다.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원래 문장: \"안녕하세요. 버트를 사용한 모델입니다.\"\n",
    "text = \"안녕하세요. 버트를 사용한 모델입니다.\"\n",
    "encoded = tokenizer.encode(text)\n",
    "\n",
    "# 토크나이즈한 후 토큰의 id를 출력하라 \n",
    "token_ids = encoded.ids\n",
    "\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "F_5JG3PP5AGP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'베이커리'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전에 없는 단어는 어떻게 디코딩되는가?\n",
    "tokenizer.decode(unknown_token_ids)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week3_2_assginment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wanted",
   "language": "python",
   "name": "wanted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
